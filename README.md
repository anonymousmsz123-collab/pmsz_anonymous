# pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse–Smale Segmentations in Lossy Compression

This repository provides an **anonymous implementation** of **pMSz**, a distributed-memory, GPU-accelerated algorithm for correcting extrema and piecewise linear Morse–Smale segmentations (PLMSS) in error-bounded lossy compression.


### Files

- **`pMSz.cu`**  
  The proposed distributed parallel algorithm described in the paper.

- **`sync_pMSz.cu`**  
  A baseline variant that synchronizes ghost layers after every iteration, used to study synchronization overhead.

- **`naive_MSz.cu`**  
  A direct distributed-memory parallelization of MSz that recomputes integral paths, included as a scalability baseline.

---
## Synthetic Data Generation (Perlin Noise)

This repository includes a CUDA-based Perlin noise data generator (`generate_perlin.py`) for producing
synthetic 3D datasets used in the scalability experiments reported in pMSz.

The generator follows a weak-scaling setup, where each MPI rank is associated with a fixed local
subdomain of size 512 × 512 × 512. For a given number of ranks, the global domain size is determined
by a three-dimensional process decomposition, and Perlin noise values are generated consistently
across the entire global domain using global offsets.

### How to Run

The Perlin noise generator can be executed directly as a standalone script. CUDA support is required.

```bash
python generate_perlin.py
```


## Dependencies

- CUDA Toolkit (`nvcc`)
- MPI with CUDA-aware support (`mpicxx`)
- C++17 compatible compiler
- Zstandard (`zstd`)
- ZFP
- SZ3
- Linux-based HPC system (e.g., SLURM)

---

## Compilation

The code is compiled using `nvcc` with MPI as the host compiler.  
Example compilation command:

```bash
nvcc -ccbin mpicxx -std=c++17 pMSz.cu -o pmsz \
    -lzstd -lzfp -lsz3
```
---

## Running the Code

The program is intended to be executed in a distributed multi-GPU environment using MPI.  
In our experiments, we use `srun` on SLURM-based systems.

### Command Format

```bash
srun -n <num_mpi_ranks> ./pmsz \
    <dataset_path> \
    <dim_x> <dim_y> <dim_z> \
    <abs_error_bound> \
    <compressor_name> \
    <mpi_dim_x> <mpi_dim_y> <mpi_dim_z>
```

### Parameters

- **num_mpi_ranks**  
  Total number of MPI processes. Must satisfy  
  `num_mpi_ranks = mpi_dim_x × mpi_dim_y × mpi_dim_z`.

- **dataset_path**  
  Path to the input 3D scalar field.

- **dim_x dim_y dim_z**  
  Global dataset dimensions.

- **error_bound**  
  Error bound used by the base compressor.

- **compressor_name**  
  Name of the base compressor. Supported options:
  - `sz3`
  - `zfp`

- **mpi_dim_x mpi_dim_y mpi_dim_z**  
  MPI process grid dimensions in x, y, and z directions.

---

## Example

```bash
srun -n 8 ./pmsz data.bin 256 256 256 1e-3 sz3 2 2 2
```

---

## Experimental Setup 

- **Hardware:** NERSC Perlmutter system with NVIDIA A100 GPUs  
- **Scaling:** Weak scaling up to 128 GPUs; strong scaling up to 16 GPUs  
- **Compressors:** SZ3 and ZFP  
- **Baselines:** MSz (single GPU), naive-MSz, sync-pMSz  
- **Datasets:** Synthetic (Perlin noise) and real-world scientific datasets  

---

## Notes on Experimental Results

The tables and figures reported in the paper are generated by running the provided implementations
(`pMSz`, `sync_pMSz`, and `naive_MSz`) under the experimental configurations described in Section V
of the paper. Scripts and detailed workflows for reproducing the tables and figures will be provided.
